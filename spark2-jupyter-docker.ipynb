{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Spark setup\n",
    "\n",
    "This notebook is meant to run on a spark 2 docker container. First i'll describe the steps to set it up.\n",
    "\n",
    "On a Linux based system install Docker and Docker-compose.Create this file : docker-compose.yml. The contents is listed below.  Then run: docker-compose build . Afterwards run this command : docker-compose build -d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "version: \"2\"\n",
    "\n",
    "services:\n",
    "  master:\n",
    "    image: singularities/spark\n",
    "    command: start-spark master\n",
    "    hostname: master\n",
    "    ports:\n",
    "      - \"6066:6066\"\n",
    "      - \"7070:7070\"\n",
    "      - \"8080:8080\"\n",
    "      - \"50070:50070\"\n",
    "      - \"8888:8888\"\n",
    "  worker:\n",
    "    image: singularities/spark\n",
    "    command: start-spark worker master\n",
    "    environment:\n",
    "      SPARK_WORKER_CORES: 1\n",
    "      SPARK_WORKER_MEMORY: 2g\n",
    "    links:\n",
    "      - master\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "With docker ps , check if the master and worker containers are running.\n",
    "Connect to the master node:\n",
    "docker exec -it [container id master] bash\n",
    "On the master node continue with setting up as described below.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark and conda env setup\n",
    "\n",
    "```\n",
    "First install Anaconda 4 (latest version) on the Docker container with Spark Master. Then install a new Conda environment for Spark, using python 3.5 (3.6 has a bug).  \n",
    "\n",
    "conda create -n spark python=3.5\n",
    "source activate spark\n",
    "conda install notebook ipykernel\n",
    "ipython kernel install --user --name spark --display-name spark\n",
    "\n",
    "Make jupyter start script, and run it:\n",
    "PYSPARK_PYTHON=/root/anaconda3/envs/spark/bin/python\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=0.0.0.0 --port=8888' $SPARK_HOME/bin/pyspark\n",
    "\n",
    "Now go to the url it gives (http://0.0.0.0:8888/<some code>)\n",
    ", Run the nodebook sections.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start this in spark conda env to test\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data\n",
    "\n",
    "```\n",
    "This example works if you clone https://github.com/PacktPublishing/Learning-PySpark\n",
    "\n",
    "and make sure its in /root/learningPySpark on the Docker container with Spark Master. \n",
    "\n",
    "To install git on this container run command: apt-get install git\n",
    ", on github (or bitbucket) create a repository so you can save changes from the container and push it to Github. Use the following commands on the Docker container to init and push the data :\n",
    "\n",
    "git init\n",
    "git add <your file>\n",
    "git commit -m \"first commit\"\n",
    "git remote add origin https://github.com/michelnossin/pyspark_training_docker.git\n",
    "git push -u origin master\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execute this section to test Spark\n",
    "flights = \"file:/root/learningPySpark/Chapter03/flight-data/departuredelays.csv\" \n",
    "airports = \"file:/root/learningPySpark/Chapter03/flight-data/airport-codes-na.txt\" \n",
    "airports_df = spark.read.csv(airports,header='true',inferSchema='true',sep='\\t')\n",
    "airports_df.createOrReplaceTempView(\"airports\")\n",
    "flights_df = spark.read.csv(flights,header='true')\n",
    "flights_df.createOrReplaceTempView(\"flights\")\n",
    "flights_df.cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
